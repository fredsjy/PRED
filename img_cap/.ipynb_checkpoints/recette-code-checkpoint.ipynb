{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# source code\n",
    "### normData.py \n",
    "### caption_accuracy.py\n",
    "### image_cnn.py\n",
    "### tsne_viz.py\n",
    "# file \n",
    "### normData : Instance of Class normData, having all the data of captions, images, labels\n",
    "### cap_vec : Result after Doc2Vec and NN, 1000 vectors of 20 dimensions\n",
    "### fc1_pre : 1000 vectors of 20 dimensions, using just one full-connected layer in CNN\n",
    "### fc2_pre : 1000 vectors of 20 dimensions, using two full-connected layer in CNN\n",
    "### fc2_cap_40dim_viz.png : result of visualization using union of fc2_pre and cap_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # normData.py\n",
    " ### use a class to generate the elements of data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import namedtuple\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class NormData(object):\n",
    "    def __init__(self, captions_directory=None, images_directory=None, label_file=None):\n",
    "        self.captions_src = captions_directory\n",
    "        self.images_src = images_directory\n",
    "        self.label_src = label_file\n",
    "        self.captions = []\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_label_dict = {}\n",
    "        if captions_directory is not None:\n",
    "            self.caps2vec()\n",
    "        if label_file is not None:\n",
    "            self.labels2vec()\n",
    "        if images_directory is not None:\n",
    "            self.img2vec()\n",
    "\n",
    "    def caps2vec(self):\n",
    "        # extract captions and file numbers as a tuple\n",
    "        file_names = [f for f in listdir(self.captions_src) if isfile(join(self.captions_src, f))]\n",
    "        namedtuple_file = namedtuple('namedtuple_file', 'doc number')\n",
    "        captions_names = []\n",
    "        for file in file_names:\n",
    "            with open(self.captions_src + file, 'r') as f:\n",
    "                captions = f.read()\n",
    "                captions_names.append(namedtuple_file(captions, file.split('.')[0]))\n",
    "\n",
    "        # split file to words with number\n",
    "        words_number = []\n",
    "        analyzed_document = namedtuple('analyzed_document', 'words tags')\n",
    "        for document in captions_names:\n",
    "            words = document[0].replace('.','\\n').lower().split()\n",
    "            number = [document[1]]\n",
    "            words_number.append(analyzed_document(words, number))\n",
    "        # print(words_number[0])\n",
    "\n",
    "        # construct a doc2vec model\n",
    "        model = gensim.models.Doc2Vec(words_number, dbow_words=1, size=1024, window=8, min_count=5, workers=4)\n",
    "\n",
    "        # extract captions vectors and keys\n",
    "        self.captions = [None] * model.docvecs.__len__()\n",
    "        for i, vec in enumerate(model.docvecs):\n",
    "            index = int(model.docvecs.index_to_doctag(i))\n",
    "            self.captions[index-1] = vec\n",
    "\n",
    "    def labels2vec(self):\n",
    "        labels = []\n",
    "        with open(self.label_src, 'r') as labels_file:\n",
    "            line = labels_file.readline().rstrip()\n",
    "            while line:\n",
    "                labels.append(line)\n",
    "                line = labels_file.readline().rstrip()\n",
    "\n",
    "        # match label and class\n",
    "        s = set(labels)\n",
    "        l = list(s)\n",
    "        l.sort()\n",
    "        class_label_dict = {}\n",
    "        i = 1\n",
    "        for e in l:\n",
    "            class_label_dict[e] = i\n",
    "            i += 1\n",
    "\n",
    "        self.class_label_dict = class_label_dict.copy()\n",
    "\n",
    "        # construct labels list\n",
    "        for label in labels:\n",
    "            label_vec = [0] * len(s)\n",
    "            label_vec[class_label_dict[label]-1] = 1\n",
    "            self.labels.append(label_vec)\n",
    "\n",
    "    def img2vec(self):\n",
    "        img_names = [f for f in listdir(self.images_src) if isfile(join(self.images_src, f))]\n",
    "        for img_name in img_names:\n",
    "            img = io.imread(self.images_src+img_name)\n",
    "            img_resize = resize(img, (28, 28))\n",
    "            # r = img_resize[:, :, 0].flatten()\n",
    "            # g = img_resize[:, :, 1].flatten()\n",
    "            # b = img_resize[:, :, 2].flatten()\n",
    "            # image = np.append(r,g)\n",
    "            # image = np.append(image,b)\n",
    "            self.images.append(img_resize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# caption_accuracy.py\n",
    "### construct the neural network for captions\n",
    "### test the accuracy of the captions after Doc2Vec\n",
    "### get the vectors using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ,accuracy: 0.278\n",
      "200 ,accuracy: 0.398\n",
      "400 ,accuracy: 0.421\n",
      "600 ,accuracy: 0.437\n",
      "800 ,accuracy: 0.456\n",
      "1000 ,accuracy: 0.469\n",
      "1200 ,accuracy: 0.485\n",
      "1400 ,accuracy: 0.502\n",
      "1600 ,accuracy: 0.513\n",
      "1800 ,accuracy: 0.516\n",
      "2000 ,accuracy: 0.527\n",
      "2200 ,accuracy: 0.533\n",
      "2400 ,accuracy: 0.538\n",
      "2600 ,accuracy: 0.546\n",
      "2800 ,accuracy: 0.548\n",
      "3000 ,accuracy: 0.551\n",
      "3200 ,accuracy: 0.554\n",
      "3400 ,accuracy: 0.556\n",
      "3600 ,accuracy: 0.56\n",
      "3800 ,accuracy: 0.564\n",
      "4000 ,accuracy: 0.566\n",
      "4200 ,accuracy: 0.568\n",
      "4400 ,accuracy: 0.569\n",
      "4600 ,accuracy: 0.573\n",
      "4800 ,accuracy: 0.573\n",
      "5000 ,accuracy: 0.576\n",
      "5200 ,accuracy: 0.575\n",
      "5400 ,accuracy: 0.577\n",
      "5600 ,accuracy: 0.579\n",
      "5800 ,accuracy: 0.581\n",
      "6000 ,accuracy: 0.582\n",
      "6200 ,accuracy: 0.586\n",
      "6400 ,accuracy: 0.585\n",
      "6600 ,accuracy: 0.585\n",
      "6800 ,accuracy: 0.588\n",
      "7000 ,accuracy: 0.588\n",
      "7200 ,accuracy: 0.59\n",
      "7400 ,accuracy: 0.589\n",
      "7600 ,accuracy: 0.588\n",
      "7800 ,accuracy: 0.588\n",
      "8000 ,accuracy: 0.588\n",
      "8200 ,accuracy: 0.589\n",
      "8400 ,accuracy: 0.59\n",
      "8600 ,accuracy: 0.59\n",
      "8800 ,accuracy: 0.589\n",
      "9000 ,accuracy: 0.589\n",
      "9200 ,accuracy: 0.591\n",
      "9400 ,accuracy: 0.59\n",
      "9600 ,accuracy: 0.591\n",
      "9800 ,accuracy: 0.593\n",
      "10000 ,accuracy: 0.594\n",
      "10200 ,accuracy: 0.595\n",
      "10400 ,accuracy: 0.595\n",
      "10600 ,accuracy: 0.594\n",
      "10800 ,accuracy: 0.594\n",
      "11000 ,accuracy: 0.595\n",
      "11200 ,accuracy: 0.595\n",
      "11400 ,accuracy: 0.595\n",
      "11600 ,accuracy: 0.596\n",
      "11800 ,accuracy: 0.596\n",
      "12000 ,accuracy: 0.596\n",
      "12200 ,accuracy: 0.596\n",
      "12400 ,accuracy: 0.597\n",
      "12600 ,accuracy: 0.597\n",
      "12800 ,accuracy: 0.597\n",
      "13000 ,accuracy: 0.598\n",
      "13200 ,accuracy: 0.599\n",
      "13400 ,accuracy: 0.599\n",
      "13600 ,accuracy: 0.6\n",
      "13800 ,accuracy: 0.601\n",
      "14000 ,accuracy: 0.602\n",
      "14200 ,accuracy: 0.602\n",
      "14400 ,accuracy: 0.601\n",
      "14600 ,accuracy: 0.601\n",
      "14800 ,accuracy: 0.601\n",
      "15000 ,accuracy: 0.602\n",
      "15200 ,accuracy: 0.604\n",
      "15400 ,accuracy: 0.606\n",
      "15600 ,accuracy: 0.606\n",
      "15800 ,accuracy: 0.607\n",
      "16000 ,accuracy: 0.609\n",
      "16200 ,accuracy: 0.61\n",
      "16400 ,accuracy: 0.609\n",
      "16600 ,accuracy: 0.61\n",
      "16800 ,accuracy: 0.61\n",
      "17000 ,accuracy: 0.61\n",
      "17200 ,accuracy: 0.611\n",
      "17400 ,accuracy: 0.613\n",
      "17600 ,accuracy: 0.615\n",
      "17800 ,accuracy: 0.615\n",
      "18000 ,accuracy: 0.616\n",
      "18200 ,accuracy: 0.617\n",
      "18400 ,accuracy: 0.616\n",
      "18600 ,accuracy: 0.616\n",
      "18800 ,accuracy: 0.617\n",
      "19000 ,accuracy: 0.616\n",
      "19200 ,accuracy: 0.616\n",
      "19400 ,accuracy: 0.616\n",
      "19600 ,accuracy: 0.615\n",
      "19800 ,accuracy: 0.615\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# load datasets\n",
    "# n = norm_dataset.NormData(captions_directory=\"../data/pascal-sentences/ps_captions/\"\n",
    "#                           ,label_file=\"../data/pascal-sentences/labels.txt\")\n",
    "#                           #,images_directory=\"../data/pascal-sentences/ps_images/\")\n",
    "\n",
    "# n = pickle.load(open(\"../captions_vectors_nn/normData\", \"rb\"))\n",
    "n = pickle.load(open(\"normData\",\"rb\"))\n",
    "\n",
    "train_data_nd = np.array(n.captions)\n",
    "\n",
    "train_label_nd = np.array(n.labels)\n",
    "\n",
    "data_nd = np.array(n.captions)\n",
    "label_nd = np.array(n.labels)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "for i in range(1000):\n",
    "    if i % 10 < 10:\n",
    "        train_data.append(data_nd[i])\n",
    "        train_label.append(label_nd[i])\n",
    "    else:\n",
    "        test_data.append(data_nd[i])\n",
    "        test_label.append(label_nd[i])\n",
    "\n",
    "\n",
    "test_data_nd = np.array(test_data)\n",
    "test_label_nd = np.array(test_label)\n",
    "\n",
    "\n",
    "# implement the regression\n",
    "captions_dim = 1024\n",
    "x = tf.placeholder(tf.float32,[None,captions_dim])\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.zeros([1024, 20]))\n",
    "b1 = tf.Variable(tf.zeros([20]))\n",
    "y1 = tf.matmul(x, W1) + b1\n",
    "\n",
    "\n",
    "# output\n",
    "y = y1\n",
    "y_ = tf.placeholder(tf.float32,[None,20])\n",
    "\n",
    "# cross_entropy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train\n",
    "for i in range(20000):\n",
    "    sess.run(train_step, feed_dict={x: train_data_nd, y_: train_label_nd})\n",
    "    if i % 200 == 0:\n",
    "        print(i, ',accuracy:', sess.run(accuracy, feed_dict={x: train_data_nd, y_: train_label_nd}))\n",
    "\n",
    "\n",
    "# get the captions vectors\n",
    "# cap_vec = sess.run(tf.nn.softmax(y),feed_dict={x:train_data_nd})\n",
    "# pickle.dump(cap_vec,open('cap_vec','wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# images_cnn.py\n",
    "### to construct a convolutional neural network to train with the images and test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#\n",
    "# n = norm_dataset.NormData(images_directory=\"../data/pascal-sentences/ps_images/\"\n",
    "#                           ,label_file=\"../data/pascal-sentences/labels.txt\"\n",
    "#                           ,captions_directory=\"../data/pascal-sentences/ps_captions/\")\n",
    "# pickle.dump(n,open('normData','wb'))\n",
    "\n",
    "n = pickle.load(open(\"normData\",\"rb\"))\n",
    "\n",
    "data_nd = np.array(n.images)\n",
    "label_nd = np.array(n.labels)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "for i in range(1000):\n",
    "    if i % 10 < 6:\n",
    "        train_data.append(data_nd[i])\n",
    "        train_label.append(label_nd[i])\n",
    "    else:\n",
    "        test_data.append(data_nd[i])\n",
    "        test_label.append(label_nd[i])\n",
    "\n",
    "train_image = np.array(train_data)\n",
    "train_label = np.array(train_label)\n",
    "test_image = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "def shuffle_dataset(images, labels):\n",
    "    image_shuffle = []\n",
    "    label_shuffle = []\n",
    "    container = []\n",
    "    for i in range(len(images)):\n",
    "        container.append((images[i],labels[i]))\n",
    "    random.shuffle(container)\n",
    "    for i in range(len(images)):\n",
    "        x,y= container[i]\n",
    "        image_shuffle.append(x)\n",
    "        label_shuffle.append(y)\n",
    "    image_shuffle = np.array(image_shuffle, np.float32)\n",
    "    label_shuffle = np.array(label_shuffle, np.float32)\n",
    "    return image_shuffle, label_shuffle\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, 28,28,3])  # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 20])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "x_image = xs\n",
    "\n",
    "## conv1 layer ##\n",
    "W_conv1 = weight_variable([5, 5, 3, 16])  # patch 5x5, in size 1, out size 32\n",
    "b_conv1 = bias_variable([16])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)    # output size 32个56x56\n",
    "h_pool1 = max_pool_2x2(h_conv1)  # output size 32个14x14\n",
    "\n",
    "## conv2 layer ##\n",
    "W_conv2 = weight_variable([5, 5, 16, 32])  # patch 5x5, in size 32, out size 64\n",
    "b_conv2 = bias_variable([32])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  # output size 14x14x64\n",
    "h_pool2 = max_pool_2x2(h_conv2)  # output size 64个7x7\n",
    "\n",
    "## if use fc1, use like this. if use fc2, replace the following code with \"fc1 layer and fc2 layer\"##\n",
    "## fc1 layer ##\n",
    "# h_pool2_flat = tf.reshape(h_pool2, [-1,32*7*7])\n",
    "# W_fc1 = weight_variable([32*7*7, 20])\n",
    "# b_fc1 = bias_variable([20])\n",
    "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# prediction = tf.nn.softmax(h_fc1)\n",
    "\n",
    "## fc1 layer ##\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,32*7*7])\n",
    "W_fc1 = weight_variable([32*7*7, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "## fc2 layer ##\n",
    "W_fc2 = weight_variable([1024, 20])\n",
    "b_fc2 = bias_variable([20])\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))  # loss\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "if int((tf.__version__).split('.')[1]) < 12:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    sess.run(train_step,feed_dict={xs: data_nd,ys: label_nd, keep_prob: 0.5})\n",
    "    if i % 10 == 0:\n",
    "        accuracy = 100 * compute_accuracy(data_nd, label_nd)\n",
    "        print(accuracy)\n",
    "\n",
    "\n",
    "# get the vectors of fc2\n",
    "# fc2_pre = sess.run(prediction,feed_dict={xs:data_nd,keep_prob:0.5})\n",
    "# pickle.dump(fc2_pre,open('fc2_pre','wb'))\n",
    "\n",
    "# get the vectors of fc1\n",
    "# fc1_pre = sess.run(prediction,feed_dict={xs:data_nd,keep_prob:0.5})\n",
    "# pickle.dump(fc1_pre,open('fc1_pre','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsne_viz.py\n",
    "### including the union of the vectors, t-sne and the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "# load the vectors\n",
    "fc2_pre = pickle.load(open('fc2_pre','rb'))\n",
    "cap_vec = pickle.load(open('cap_vec','rb'))\n",
    "\n",
    "captions_images_vectors = np.array([None]*1000)\n",
    "\n",
    "# union the vectors\n",
    "for i in range(len(captions_images_vectors)):\n",
    "    captions_images_vectors[i] = np.append(cap_vec[i],fc2_pre[i])\n",
    "\n",
    "# tsne\n",
    "tsne = TSNE(perplexity=10, n_components=2, n_iter=5000, random_state=0)\n",
    "low_dim_embs = tsne.fit_transform(list(captions_images_vectors[:]))\n",
    "\n",
    "# reshape\n",
    "low_dim_embs_rs = low_dim_embs.reshape((20,50,2))\n",
    "\n",
    "\n",
    "# define the visualization function\n",
    "colorbar = [\n",
    "    '#e4007f', '#a40000','#ea68a2','#a84200','#f19149',\n",
    "    '#fff45c','#8fc31f','#009944','#00736d','#0075a9',\n",
    "    '#004986','#500047','#b28850','#81511c','#6a3906',\n",
    "    '#59493f','#616e81','#898989','#89c997','#000000']\n",
    "\n",
    "def visualization(data):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    i=0\n",
    "    for color in colorbar:\n",
    "        x = data[i][:,0]\n",
    "        y = data[i][:,1]\n",
    "        scale = 20\n",
    "        ax.scatter(x, y, c=color, s=scale, label=\"class\"+str(i+1),alpha=1, edgecolors='none')\n",
    "        ax.legend()\n",
    "        i=i+1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# visualization\n",
    "visualization(low_dim_embs_rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
